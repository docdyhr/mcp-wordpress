name: MCP Tools Evaluation

on:
  # Run on pull requests
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'src/tools/**'
      - 'src/client/**'
      - 'evaluations/**'
      - '.github/workflows/mcp-evaluations.yml'
  
  # Run on push to main
  push:
    branches:
      - main
    paths:
      - 'src/tools/**'
      - 'src/client/**'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      eval_config:
        description: 'Evaluation config file to use'
        required: false
        default: 'ci-eval.yaml'
      
  # Schedule weekly evaluations
  schedule:
    - cron: '0 2 * * 1' # Every Monday at 2 AM UTC

jobs:
  evaluate:
    name: Evaluate MCP Tools
    runs-on: ubuntu-latest
    
    # Only run if OPENAI_API_KEY is available
    if: ${{ github.event_name == 'workflow_dispatch' || (github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository) }}
    
    strategy:
      matrix:
        node-version: [20.x]
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
      
      - name: Install Dependencies
        run: npm ci
      
      - name: Build Project
        run: npm run build
      
      - name: Setup Test WordPress Environment
        if: github.event_name != 'pull_request'
        run: |
          # This would ideally spin up a test WordPress instance
          # For now, we'll use environment variables for a test site
          echo "Setting up test environment..."
      
      - name: Run MCP Evaluations
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          TEST_WORDPRESS_URL: ${{ secrets.TEST_WORDPRESS_URL }}
          TEST_WORDPRESS_USER: ${{ secrets.TEST_WORDPRESS_USER }}
          TEST_WORDPRESS_PASSWORD: ${{ secrets.TEST_WORDPRESS_PASSWORD }}
          # Fallback to existing config if test credentials not set
          WORDPRESS_SITE_URL: ${{ secrets.TEST_WORDPRESS_URL || secrets.WORDPRESS_SITE_URL }}
          WORDPRESS_USERNAME: ${{ secrets.TEST_WORDPRESS_USER || secrets.WORDPRESS_USERNAME }}
          WORDPRESS_APP_PASSWORD: ${{ secrets.TEST_WORDPRESS_PASSWORD || secrets.WORDPRESS_APP_PASSWORD }}
          NODE_ENV: test
        run: |
          # Create results directory
          mkdir -p evaluations/results
          
          # Run evaluations using mcp-eval and capture output
          npx mcp-eval \
            evaluations/config/${{ github.event.inputs.eval_config || 'ci-eval.yaml' }} \
            dist/index.js > evaluations/results/raw-output.txt 2>&1 || true
          
          # Extract evaluation results from the output
          if [ -f "evaluations/results/raw-output.txt" ]; then
            # Extract JSON evaluations from the output
            node -e "
              const fs = require('fs');
              const content = fs.readFileSync('evaluations/results/raw-output.txt', 'utf8');
              const lines = content.split('\\n');
              
              const evaluations = [];
              let currentEval = null;
              let inJson = false;
              let jsonContent = '';
              
              for (const line of lines) {
                if (line.includes(':') && line.match(/^[a-zA-Z_]+:$/)) {
                  if (currentEval && jsonContent) {
                    try {
                      const parsed = JSON.parse(jsonContent);
                      evaluations.push({ name: currentEval, ...parsed });
                    } catch (e) {}
                  }
                  currentEval = line.replace(':', '').trim();
                  jsonContent = '';
                  inJson = false;
                } else if (line.trim() === '{') {
                  inJson = true;
                  jsonContent = '{';
                } else if (inJson) {
                  jsonContent += line + '\\n';
                  if (line.trim() === '}') {
                    inJson = false;
                    if (currentEval) {
                      try {
                        const parsed = JSON.parse(jsonContent);
                        evaluations.push({ name: currentEval, ...parsed });
                      } catch (e) {}
                    }
                  }
                }
              }
              
              fs.writeFileSync('evaluations/results/evaluation-results.json', 
                JSON.stringify({ evaluations }, null, 2));
            " || {
              echo "Failed to parse evaluations, creating fallback results"
              echo '{"evaluations": [], "overall_score": 0}' > evaluations/results/evaluation-results.json
            }
          else
            echo "No output from mcp-eval, creating fallback results"
            echo '{"evaluations": [], "overall_score": 0}' > evaluations/results/evaluation-results.json
          fi
      
      - name: Process Evaluation Results
        id: process_results
        run: |
          # Create evaluation summary
          if [ -f "evaluations/results/evaluation-results.json" ]; then
            node evaluations/scripts/process-results.js
          else
            echo "No evaluation results found, creating empty summary"
            echo '{"overall_score": 0, "tests_passed": 0, "total_tests": 3}' > evaluations/results/evaluation-summary.json
          fi
          
          # Set outputs for PR comment
          echo "score=$(jq -r '.overall_score' evaluations/results/evaluation-summary.json)" >> $GITHUB_OUTPUT
          echo "passed=$(jq -r '.tests_passed' evaluations/results/evaluation-summary.json)" >> $GITHUB_OUTPUT
          echo "total=$(jq -r '.total_tests' evaluations/results/evaluation-summary.json)" >> $GITHUB_OUTPUT
      
      - name: Upload Evaluation Results
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: |
            evaluations/results/evaluation-results.json
            evaluations/results/evaluation-summary.json
          retention-days: 30
      
      - name: Comment PR with Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const score = '${{ steps.process_results.outputs.score }}';
            const passed = '${{ steps.process_results.outputs.passed }}';
            const total = '${{ steps.process_results.outputs.total }}';
            
            let summaryContent = '';
            try {
              summaryContent = fs.readFileSync('evaluations/results/evaluation-summary.json', 'utf8');
            } catch (error) {
              summaryContent = '{"error": "Could not read evaluation summary"}';
            }
            
            const comment = `## ü§ñ MCP Tools Evaluation Results
            
            **Overall Score:** ${score}/5.0
            **Tests Passed:** ${passed}/${total}
            
            <details>
            <summary>View Detailed Results</summary>
            
            \`\`\`json
            ${summaryContent}
            \`\`\`
            
            </details>
            
            ${score >= 4.5 ? '‚úÖ Excellent performance!' : score >= 4.0 ? '‚úÖ Good performance' : score >= 3.5 ? '‚ö†Ô∏è Acceptable performance' : '‚ùå Performance needs improvement'}
            
            [View Full Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
      
      - name: Fail if Score Too Low
        if: github.event_name != 'pull_request'
        run: |
          score="${{ steps.process_results.outputs.score }}"
          if (( $(echo "$score < 3.5" | bc -l) )); then
            echo "‚ùå Evaluation score too low: $score/5.0"
            echo "Minimum required score: 3.5/5.0"
            exit 1
          else
            echo "‚úÖ Evaluation score: $score/5.0 (passed)"
          fi
      
      - name: Generate Performance Trends
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          # Track performance over time
          node evaluations/scripts/track-performance.js
      
      - name: Update Evaluation Badge
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          # Update README with latest evaluation score
          score="${{ steps.process_results.outputs.score }}"
          color="brightgreen"
          if (( $(echo "$score < 3.5" | bc -l) )); then
            color="red"
          elif (( $(echo "$score < 4.0" | bc -l) )); then
            color="yellow"
          elif (( $(echo "$score < 4.5" | bc -l) )); then
            color="green"
          fi
          
          # This would update a badge in README or create a gist
          echo "MCP Evaluation Score: $score - Color: $color"

  evaluate-performance:
    name: Performance Evaluation
    runs-on: ubuntu-latest
    needs: evaluate
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Download Previous Results
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: mcp-evaluations.yml
          name: evaluation-results
          path: evaluations/results/previous
        continue-on-error: true
      
      - name: Download Current Results
        uses: actions/download-artifact@v4
        with:
          name: evaluation-results
          path: evaluations/results/current
      
      - name: Compare Performance
        run: |
          # Compare with previous results
          if [ -f "evaluations/results/previous/evaluation-summary.json" ]; then
            node evaluations/scripts/compare-performance.js
          else
            echo "No previous results found for comparison"
          fi
      
      - name: Notify on Performance Regression
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: '‚ö†Ô∏è MCP Tools Performance Regression Detected',
              body: 'The latest evaluation shows a performance regression. Please review the [evaluation results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}).',
              labels: ['performance', 'automated']
            });